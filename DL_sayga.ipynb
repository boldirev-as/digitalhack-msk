{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "761fc5a2-6411-405b-8591-b5a490d25b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:56:57.611081Z",
     "iopub.status.busy": "2023-11-25T08:56:57.610116Z",
     "iopub.status.idle": "2023-11-25T08:56:57.628577Z",
     "shell.execute_reply": "2023-11-25T08:56:57.627465Z",
     "shell.execute_reply.started": "2023-11-25T08:56:57.611029Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !export PATH=$PATH:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2129e9-82e3-4ba5-876e-9b5bfd029605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:56:57.714733Z",
     "iopub.status.busy": "2023-11-25T08:56:57.713807Z",
     "iopub.status.idle": "2023-11-25T08:56:57.727997Z",
     "shell.execute_reply": "2023-11-25T08:56:57.726686Z",
     "shell.execute_reply.started": "2023-11-25T08:56:57.714699Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade requests botocore urllib3 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306fc003-12d3-4d9a-be0a-7e786ae563fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:56:57.832655Z",
     "iopub.status.busy": "2023-11-25T08:56:57.831775Z",
     "iopub.status.idle": "2023-11-25T08:56:57.853508Z",
     "shell.execute_reply": "2023-11-25T08:56:57.852315Z",
     "shell.execute_reply.started": "2023-11-25T08:56:57.832616Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install accelerate==0.21.0 \\\n",
    "#   bitsandbytes==0.40.2 \\\n",
    "#   peft==0.5.0 \\\n",
    "#   transformers==4.34.0 \\\n",
    "#   sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b763db81-1531-46fe-a105-dd09a14f1aa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:56:57.938252Z",
     "iopub.status.busy": "2023-11-25T08:56:57.937136Z",
     "iopub.status.idle": "2023-11-25T08:57:05.641871Z",
     "shell.execute_reply": "2023-11-25T08:57:05.640667Z",
     "shell.execute_reply.started": "2023-11-25T08:56:57.938207Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 08:57:04.683211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import srt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "# from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2fa6f-da3b-400e-93c6-61a2409dbb1c",
   "metadata": {},
   "source": [
    "## Чтение файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f81a6468-d65e-4c80-b8de-fd432e3b9f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.644491Z",
     "iopub.status.busy": "2023-11-25T08:57:05.643547Z",
     "iopub.status.idle": "2023-11-25T08:57:05.663593Z",
     "shell.execute_reply": "2023-11-25T08:57:05.662445Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.644455Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"new_test_results/new_test_text_5.txt\", mode='r') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71429feb-d30a-4a37-94bd-20a009034010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.666424Z",
     "iopub.status.busy": "2023-11-25T08:57:05.665617Z",
     "iopub.status.idle": "2023-11-25T08:57:05.685860Z",
     "shell.execute_reply": "2023-11-25T08:57:05.684718Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.666375Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    Всем привет! Сегодня мы продолжаем изучать веб скрейпинг и перейдем к разбору Python модуля LXML и языка xpath. В одном из предыдущих уроков мы познакомились с Beautiful Soup и использовали его для скрейпинга HTML страниц. В текущем же уроке будем работать с модулем LXML. Beautiful Soup богаче с точки зрения функциональных возможностей, но для выполнения простых задач можно использовать LXML. Сегодня наша задача научиться писать выражение xpath, которые используются для скрейпинга нужных частей информации. В первой половине урока мы займемся теоретическим разбором инструментов, а во второй части урока применим на практике полученные знания и выполним скрейпинг сайта IMDB. Я советую вам по ходу лекции повторять все, что я делаю. Откройте VS Code и воспроизводите тот же код, что и в лекции. Итак, сегодня на уроке мы разберем основы LXML, xpath, поработаем с xpath в рамках LXML, поговорим о том, что такое CSS селекторы, и наконец выполним скрейпинг сайта с помощью xpath. Основы LXML Начнем с определения. LXML – это библиотека Python для обработки документов XML и HTML. Она предоставляет быстрый и эффективный API для парсинга, манипулирования и серилизации данных XML и HTML. Основные возможности LXML включают поддержку xpath и работу с деревом элементов. Давайте перейдем в нашу среду разработки Visual Studio Code и продолжим работать там. Мы находимся в VS Code и для начала нужно установить LXML. Установка LXML стандартная, посредством PIP. Установка LXML. У меня эта библиотека уже установлена, поэтому продолжаем работу дальше. Чтобы начать работу с LXML, давайте возьмем элементарную HTML страницу и посмотрим на ее структуру. Как видно, тело страницы содержит один параграф, hello GeekBrains, один список, внутри которого находятся элементы, и второй из элементов содержит тег гиперссылки. Можно запустить этот файл прямо из среды разработки. Для этого нужно установить расширение, которое называется Live Server. Для этого перейдем на вкладку расширения. В строке поиска напишем Live Server. У меня это расширение уже установлено. После установки расширения возвращаемся на нашу страницу, жмем правую кнопку мышки и выбираем Open with Live Server. Нажимаем и наша HTML страница откроется в браузере. Пока что мы работаем с игрушечным примером, но это нужно для учебных целей. Позже мы будем работать с настоящим сайтом. Сейчас, чтобы остановить сервер, нужно кликнуть в правом нижнем углу на вот этом значке Port. Теперь давайте создадим новый Python файл в нашем корневом каталоге и назовем его AppPy. Что мы сейчас будем делать? Это с помощью LXML откроем нашу учебную веб страницу. В файле AppPy пишет код. В первую очередь нам нужно импортировать модуль, который называется E3 из библиотеки LXML. Импортируем E3. Модуль E3 имеет функцию parse, которая принимает в качестве аргумента источник, который может быть файлом или частью файла. В качестве аргумента указываем относительный путь к нашему HTML файлу. Создадим переменную E3, в которой сохраним результат. В качестве аргумента указываем путь к нашему файлу webpages. Он находится в директории src. Давайте выведем значение переменной, сохраним наш скрипт и запустим его в терминале. Функция parse возвращает объект E3. Что произошло? Функция parse берет HTML файл и преобразует его в дерево. О дереве HTML мы говорили с вами во второй лекции. Сейчас возвращаемся к этой теме. С нашей точки зрения HTML файл это текст с определенным синтаксисом внутри. В то же время HTML может быть рассмотрено как дерево элементов. Давайте выведем элементы дерева. Чтобы вывести элементы дерева, воспользуемся готовым скриптом parse3. Давайте его запустим. Я не буду погружаться в детали кода. Вы можете, используя конспект лекции, разобраться. Запустим скрипт. Итак, что мы получили? Мы получили деревовидную структуру, в которой все теги преобразованы в объекты элемент. В этой деревовидной структуре элемент HTML является корневым элементом. У него есть два дочерних элемента head и body. Элемент head имеет один дочерний элемент, title. Элемент body имеет два дочерних элемента, peul. Элемент peul имеет два дочерних элемента, li. Первый элемент li имеет атрибут id, и второй элемент li имеет атрибут class, установленный в my class. И содержит один дочерний элемент, element a. В общем, в деревовидной структуре HTML элемент считается родительским, если он имеет один или несколько дочерних элементов. А дочерний элемент – это элемент, вложенный в другой элемент. Итак, давайте представим, что мы хотим извлечь или сделать скрипинг – заголовка нашей HTML страницы. Мы можем это сделать, потому что теперь у нас есть дерево элементов, и существует метод, который позволяет извлечь заголовок из нашей веб-страницы. У нашего объекта, элемент tree, есть метод find, который принимает в качестве аргумента путь к элементу или тегу, который мы хотим извлечь. В нашей HTML разметке мы знаем, что заголовок находится внутри тега head, поэтому в методе find мы вводим строковое значение путь к заголовку head title. Давайте создадим новую переменную, назовем ее titleElement и используем метод find в качестве аргумента укажем путь к заголовку. И на этот раз выведем значение переменной titleElement. Сохраним скрипт и запустим его еще раз. Это выражение вернет элемент title – объект элемент типа title. Если теперь мы хотим вывести фактическое значение заголовка, то можем вызвать свойство text элемента title. Давайте допишем в код.txt, сохраним скрипт, очистим терминал и запустим скрипт еще раз. Пожалуйста, мы получили текстовое значение элемента title. Теперь мы можем вывести скрипт и запустим его еще раз. Теперь мы можем вывести скрипт и запустим его еще раз. Чтобы ответить на этот вопрос, давайте посмотрим код нашей веб страницы. Здесь видно, что тег title находится внутри тега head, а тег head находится внутри тега HTML. Мы всегда должны начинать с одного из прямых дочерних элементов. Например, если мы хотим выбрать тег абзаца P hello geekbrains, мы должны начать с тега body, потому что body является прямым дочерним элементом тега HTML. Выведите текст абзаца P hello geekbrains, используя метод find. Теперь поставьте видео на паузу и возвращайтесь с результатом. Итак, чтобы вывести содержимое тега P, мы воспользуемся тем же методом find, но только изменим путь к тегу абзаца. Путь будет body и наш целевой тег P. Сохраним скрипт, запустим его в терминале. Пожалуйста, мы получили текст тега P. В качестве альтернативы методу find у нас есть другой метод, find all. Аналогично тому, как мы изучали с beautiful soup, по его названию понятно, что он будет находить все совпадающие теги и возвращать их в виде списка. Например, чтобы выбрать все элементы списка, которые находятся внутри тега ul, давайте посмотрим в нашем HTML коде. Итак, мы хотим выбрать все элементы списка внутри ul, мы должны использовать метод find all, потому что если мы используем метод find, то получим только первый элемент списка. Давайте попробуем извлечь элементы li или элементы списка. Вернемся в наш скрипт и создадим переменную listItem. И используем на этот раз метод find all. И в качестве пути укажем body, список ul и элементы li. И выведем наш список элементов. Сохраняем скрипт, запускаем его. Мы получили список из двух элементов типа li. Чтобы извлечь текстовое содержимое, нужно обратиться к каждому элементу списка отдельно. Поэтому нам придется создать цикл. Создаем цикл for и в цикле проходим по каждому элементу и выводим его. И выведем текстовое содержимое. Сохраняем скрипт, запускаем его. Обратите внимание, что для первого элемента списка мы получили полное текстовое значение. Однако для второго элемента списка мы не видим содержимого тега a. Причина здесь в том, что в методе find all мы указали, что хотим получить все элементы списка и нас не интересует есть ли внутри него тег a. Чтобы решить эту проблему, нужно внутри цикла создать переменную или что-то еще. Поэтому одно быстрое решение, которое мы можем применить, это осуществить поиск внутри цикла for. Давайте создадим переменную. Еще одну внутри цикла. Назовем ее a. И внутри элемента li используем метод find для поиска тега a. Так как у нас только один элемент содержит тега, то используем оператор if. Если a не равно none, значит внутри элемента списка есть тега, содержимое которого мы соответственно и выведем. Если a is not none, то выведем его текстовое содержимое. Здесь мы это должны заключить в фигурной скобке. В противном значении просто выводим текстовое содержимое элемента a. Этот принт нам больше не нужен. Сохраняем. Я здесь не указал кавычки. Сохраняем и запускаем скрипт. Обратите внимание, что теперь мы получили текстовое содержимое тега a со словом developer. Но перед словом developer есть большой пробел. Это связано с нашей HTML разметкой, где есть пробелы перед словом developer. Чтобы очистить вывод, мы можем использовать метод python strip. Добавим в код strip. Сохраняем и запускаем наш скрипт еще раз. На этот раз все в порядке. Одним из недостатков LXML является то, что он не богат в плане методов, которые открываются нам через объекты element3. Каждый раз, когда мы хотим выбрать тег из разметки HTML, мы должны указать абсолютный путь до тега. Если мы имеем дело с большой HTML страницей, это может быть проблемой. Чтобы решить эту проблему, существует более эффективный метод выбора тегов с помощью xpath. Переходим к следующей теме. Что такое xpath? xpath означает XML path language. Язык запросов к элементам XML документа. Язык xpath используется для уникальной идентификации или адресации частей XML документа. Выражение xpath можно использовать для поиска в XML документе и извлечения информации из любой части документа. Например, элемента или атрибуто в XML называется узлом или node. Но это не означает, что мы не можем использовать его для запроса и выбора элементов или тегов из HTML страницы. Мы начнем изучение xpath с выражений. Выражение, определяющее шаблон для выбора узлов. Как мы уже обсуждали, HTML документ рассматривается как дерево узлов. В xpath мы можем выбрать элемент, используя двойной слэш и затем имя элемента. Например, если вы хотите выбрать все дивы на HTML странице, мы используем двойной слэш div. Также мы можем выбрать элементы по их атрибуту, ID или классу, добавляя две квадратные скобки, следующие за именем элемента, а затем значение атрибуто. Мы можем выбрать элемент на основе его позиции. Например, если вы хотите выбрать первый элемент списка UL, то в квадратных скобках указываем 1. Если мы хотим выбрать первый и второй элемент списка, мы должны использовать функцию position, плюс логический оператор. Например, li и в квадратных скобках указываем position равно 1 или position равно 2. Чтобы выбрать только первый элемент списка и этот первый элемент списка должен содержать текст hello, то мы используем функцию contains в xpath. Все, что мы пишем в квадратных скобках, называется предикатом или условием. У xpath есть еще одно полезное свойство. Это возможность перемещаться в дереве HTML вверх или вниз, используя то, что называется оси xpath. В xpath оси это способ выбора элементов, которые находятся относительно текущего элемента в иерархии документа. Ось это именованная связь между элементами, которая определяет направление и набор узлов для выбора. Например, ось предков, ancestor выбирает все элементы предки, то есть элементы, расположенные выше в иерархии документа, текущего элемента. А ось потомков, following sibling, выбирает все элементы потомки, которые идут после текущего элемента. Syntaxes на слайде. Мы указываем имя оси, двойное двоеточие и затем целевой элемент, который мы ищем. Для перехода вверх по дереву HTML есть четыре оси. Первая называется parent, которая возвращает родителя указанного узла. Вторая ancestor, чтобы получить всех предков определенного узла. Третья ось preceding выбирает все узлы, которые появляются перед текущим узлом за исключением предков, узлов, атрибутов и пространства имя. Четвертая preceding sibling возвращает братьев определенного элемента, то есть все элементы одного уровня до текущего узла. Для перехода вниз по дереву HTML существуют следующие оси. Ось child, которая будет получать дочерние элементы потомков определенного узла. Ось following вернет все элементы, находящиеся после закрывающего тега определенного узла. Ось following sibling возвращает все элементы одного уровня после текущего узла. И наконец ось descendant возвращает всех потомков текущего узла. Перейдем к следующей части и посмотрим, как на практике применять xpath в рамках LXML. Итак, возвращаемся в VS Code, удалим лишние строки, которые нам сейчас не нужны, оставим только дерево элементов. Для выбора тегов с помощью xpath в LXML мы можем заменить наш метод find, который мы ранее использовали, методом xpath. Метод xpath принимает в качестве аргумента путь к целевому теку точно так же, как и метод find, но особенность его состоит в том, что нам не нужно указывать полный путь для тека. Давайте попробуем выполнить парсинг заголовка. Создадим переменную titleElement. Возьмем дерево и на этот раз вместо метода find используем xpath. И в качестве аргумента просто указываем tagTitle и выведем его на печать. Метод xpath возвращает список, поэтому чтобы получить доступ к свойству текст нам нужно получить доступ к первому элементу списка. Поэтому выведем titleElement и используем первый элемент списка и возьмем текст. Сохраняем скрипт и запустим. Мы получили текстовое содержимое тега title. Далее xpath позволяет нам получить доступ к тексту без использования свойства текста, связанного с LXML, то есть без вот этого свойства. И получить тот же результат. Выполняется это следующим образом. Мы используем текст прямо в xpath выражении. И также получаем доступ к первому его элементу. Соответственно вот эта часть кода нам не нужна. Выводим только содержимое значение переменной titleElement. Давайте очистим терминал и запустим еще раз наш скрипт. Мы получили тот же самый результат. Задание. Попробуйте самостоятельно получить доступ к тексту tgp и вывести текст hellogeekbrains. Поставьте видео на паузу и возвращайтесь, когда сделаете задание. Возвращаемся в VSCode. Давайте теперь получим данные из списка в html документе с помощью xpath. Только на этот раз не будем использовать функцию текст, а воспользуемся методом toString. Создадим переменную ListItems. Используем метод xpath для выбора всех элементов li. Далее в цикле for. Для каждого ListItems выведем его значение. Используя метод toString в качестве аргумента используем li. Сохраняем скрипт и запускаем. После выполнения кода мы увидим, что получили два списка элементов, содержащих полную html разметку. Чтобы получить текст, который находится внутри элементов списка, давайте создадим переменную в цикле, которая будет применять функцию xpath.text к каждому элементу списка. Создадим переменную text, которая будет брать каждый элемент li, применять к нему метод xpath. Используя функцию text, извлекаем текст. И выводим значение переменной текст в каждой итерации. И еще раз запускаем. После выполнения кода мы снова получаем два списка, однако все еще мы получаем текст со всеми специальными символами, находящимися в html коде. Чтобы это исправить, нужно всего лишь добавить точку перед двойным slash функции text. То есть здесь мы просто ставим точку, сохраняем, запускаем скрипт еще раз. На этот раз получаем только текст двух тегов li. Пока что мы получили два списка, первые из которых содержат только один элемент с текстом, а второй содержит три элемента, но все еще содержит пробелы. Давайте избавимся от этих лишних символов. Для этого мы можем использовать метод map, который принимает в качестве первого аргумента функцию, которая будет применена к каждому элементу списка. Используем strip, чтобы удалить все пробелы и символы переноса строки. Дополним наш код. Используем map. И применим strip к каждому элементу. Далее преобразуем в список наш вывод. Очистим терминал и запускаем скрипт еще раз. Наконец, чтобы код возвращал не списки, а только текст, мы можем использовать другую функцию python.joint. И здесь в свою очередь нам уже список больше не нужен. Выводим значение, переменный текст, сохраняем скрипт и запускаем его в терминале. Мы получили чистый текст. Наконец, давайте посмотрим пример работы с осями xp. Выберем все элементы li, которые являются потомками элемента ul. Если я говорю немного непонятно, возвращайтесь к HTML коду и сверяйтесь с исходным HTML кода. Итак, меняем выражение xpath, чтобы выбрать всех потомков тега ul. Для этого используется слово descendant li. Сохраняем, запускаем скрипт. Вы можете свериться с HTML и убедиться, что мы выбрали все элементы li, которые являются потомками элемента ul. Таким образом, xpath – это мощный инструмент для выбора и навигации по элементам в документах XML и HTML, который особенно полезен для задач вебскрейпинга и извлечения данных. В этой части лекции мы рассмотрели основы использования xpath.vl.xml для выбора и извлечения данных из HTML документов. А сейчас давайте познакомимся с еще одним инструментом для извлечения данных из HTML кода – CSS селектора. Как вы знаете, CSS служит для стилизации HTML страниц. Идея состоит в том, что для скрейпинга мы можем использовать lxml вместе с CSS селекторами для скрейпинга тега или элемента стилизации веб-страниц. Для использования CSS селекторов нам нужно установить модуль под названием CSS SELECT. Устанавливается он в терминале посредством pip CSS SELECT. Здесь ничего особенного, у меня он уже установлен. Теперь в нашем коде мы можем использовать метод CSS SELECT, который принимает в качестве аргумента CSS SELECT. Например, чтобы выбрать заголовок из HTML страницы, все что нам нужно сделать – это вызвать tag title. Давайте в очередной раз перепишем наш скрипт. Создадим переменную title element и на этот раз используем метод CSS SELECT. В качестве аргумента указываем title и выведем значение, которое также будет представлено в виде списка. Поэтому берем только нулевой элемент или первый элемент, сохраняем скрипт и запускаем наш код. Если выполнить файл в таком виде, то мы, как вы видите, получили ошибку – attribute error. Это связано с тем, что CSS SELECT работает непосредственно с HTML элементами, а не с объектом element3, как это делает xp. Поэтому предварительно нужно конвертировать element3. Создадим для этого новую переменную, назовем ее HTML. И воспользуемся функцией getRoot. Эта функция конвертирует объект дерево в HTML элемент. Сохраняем наш скрипт, очистим терминал и запускаем скрипт. И здесь я снова получил ошибку attribute error. У объекта нет атрибута CSS SELECT, это связано с тем, что здесь нужно 3 поменять на HTML. Исправляем, сохраняем и запускаем еще раз. На этот раз все в порядке. Мы получили текстовое содержимое тега title. Задание. Используйте метод CSS SELECT вместо xpath и выберите тег абзаца. Поставьте видео на паузу и после выполнения возвращайтесь. Итак CSS SELECT и xpath это два разных языка запросов, используемых для извлечения данных из документов HTML и XML. Хотя оба языка служат схожим целям, у них есть несколько ключевых различий, которые делают их подходящими для разных случаев использования. xpath более мощный и гибкий язык запросов, чем CSS SELECT, поскольку он позволяет использовать более сложные выражения и поддерживает более широкий спектр типов данных и функций. xpath также поддерживает оси, которые позволяют перемещаться по иерархии дерева и выбирать элементы на основе их связи с другими элементами. CSS SELECT с другой стороны является более простым и лаконичным языком запросов. CSS SELECT может быть полезен для задач вебскрейпинга, связанных с выбором элементов на основе их класса, ID или других CSS SELECT. И часто для таких задач его просто проще понимать и использовать, чем xpath. Одним из основных преимуществ CSS SELECT перед xpath является то, что он может быть быстрее и эффективнее для определенных типов выбора, особенно при выборе на основе атрибутов класса или ID. Однако важно отметить, что xpath является более мощным и гибким языком запросов и лучше подходит для более сложных задач извлечения данных, которые включают выбор элементов на основе их текстового содержимого, структуры и других атрибутов. В целом выбор языка зависит от конкретных требований задачи извлечения данных. А сейчас давайте перейдем к скрейпингу веб-сайта с помощью xpath. Мы переходим к следующему разделу лекций и займемся скрейпингом сайта IMDB, а именно страницы стоп наиболее популярных фильмов. Познакомимся с некоторыми хитростями, которые помогут вам скрейпить веб-страницы, избавимся от ненужных классов, внедренных в JavaScript, а также сохраним скрипированные данные в базе данных MongoDB. Давайте напишем код, который будет извлекать из приведенной таблицы название, место в рейтинге, изменение в место в рейтинге на странице наиболее популярных фильмов. Как вы знаете, большинство сайтов используют JavaScript. Проблема состоит в том, что библиотека requests из Python, с которой мы уже знакомы, не понимает, не видит JavaScript. Поэтому высокие шансы, что мы не получим ту же самую информацию, которую видим при отображении страницы в браузере. Поэтому, чтобы исключить JavaScript из разметки, можно использовать следующий способ. В Chrome нажимаем три вертикальных кнопки в верхнем углу. Переходим в настройки. В настройках нужно найти в поиске элемент с названием JavaScript, или просто можно написать Java. Здесь находим настройки сайтов. Переходим в настройки. И в настройках ищем Content. В нем раздел JavaScript. Переходим в него. Здесь мы видим переключатель, который позволяет разрешить сайтом использовать JavaScript, либо запретить сайтом использовать JavaScript. Сейчас мы запрещаем использовать JavaScript. Возвращаемся к нашей целевой странице, и ее нужно обновить. После обновления мы видим, что некоторые элементы пропали, например, ваш рейтинг. И, в общем-то, это тот контент, который видит библиотека Requests Python. Давайте перейдем в нашу среду разработки. В среде разработки создадим новый файл. Назовем его, например, app2.py. И давайте для начала выполним запрос get к нашему целевому сайту. Для этого для начала импортируем библиотеку Requests. Создадим переменную с ответом, назовем ее resp. И воспользуемся методом get, который принимает аргументы URL с значением URL адреса нашего целевого сайта. Скопируем адрес, возвращаемся в S-код, вставляем сюда адрес. И давайте укажем еще один аргумент. Это будет заголовок. И в фигурных скобках укажем user-agent или агент пользователя. Где взять агент пользователя? Одним из самых простых способов является загуглить его. Давайте вернемся в Chrome. И прежде чем получить наш агент пользователя, нужно разрешить сайтом использовать JavaScript. И в поиске просто пишем my user-agent. И в Chrome отображается наш агент пользователя. Копируем его отсюда, возвращаемся в S-код и указываем его в аргументе header. Указание вашего агента пользователя в аргументе запроса является хорошим тоном по ряду причин. Во-первых, некоторые веб-сайты могут блокировать запросы, которые не имеют действительного агента пользователя. Во-вторых, некоторые сайты могут предоставлять различное содержимое или различную HTML-разметку в зависимости от пользовательского агента, запрашивающего браузера. Наконец, некоторые сайты могут иметь соглашение об использовании, которое запрещает скрейпинг или извлечение данных. Указывая действительный пользовательский агент в строке запроса, вы идентифицируете себя как законного пользователя сайта, а не скрейпера или бота, который может нарушать условия обслуживания. И давайте выведем значение ответа сервера, а точнее статус код, чтобы проверить, что мы получили ответ от сервера. Сохраняем скрипт, запустим его. И здесь мы получили ошибку в связи с тем, что опечатка в аргументе headers, добавим s, сохраняем и запускаем еще раз. Итак, мы получили ответ 200, то есть успешный ответ сервера. Давайте начнем с построения дерева. Для этого нам нужно импортировать HTML из LXML. И теперь попробуем получить переменную дерева. Назовем ее 3. Используем функцию fromString и в качестве аргумента используем свойство контент нашего get ответа от сервера. Данная строка создает парсинг дерева LXML из содержимого объекта ответа. Метод fromString используется для создания нового объекта элемент из HTML, содержимого ответа, который хранится в атрибуте контент. Сохраним. И сейчас давайте вернемся к сайту. Для начала убедимся, что JavaScript отключен. Теперь кликаем правой кнопкой мышки на странице, выбираем просмотреть код и переходим к инструменту разработчика. Давайте для начала найдем название первого фильма. Для этого выберем инструмент поиска, наводим на название первого фильма и давайте посмотрим на HTML код. Как вы видите, тег A находится внутри тела таблицы, который в свою очередь находится внутри таблицы. Тейбл, который содержит два класса, chart full width и data color name. Нам нужно получить доступ к этому тексту, к названию фильма, поэтому давайте начнем писать xpath выражение. Нажимаем CTRL-F и в появившейся строке поиска вводим xpath выражение. Двойной слайш. Сначала нам нужно получить доступ к тегу table. Далее в квадратных скобках указываем значение атрибутов. Если сейчас у меня щелкнуть дважды на значение атрибута, можно его скопировать и перенести в строку поиска. Итак, мы хотим найти значение data color name. Равным. Вставляем скопированные значения, а именно chart movie matter. Как видите, в строке поиска нам указывают количество элементов с такими характеристиками. В нашем случае 1, 1 потому что таблица у нас одна. Далее внутри этой таблицы ищем следующий тег, а именно tbody. У нас тоже найден один элемент. Как видите, мы не должны указывать весь полностью путь. Мы указываем только те дочерние элементы, которые содержит целевой наш тег с названием фильма. Следующий элемент у нас tr. tr это строка. И сейчас, как вы видите, у нас найдено 100 таких элементов. 100 потому что на этой странице указано 100 наиболее популярных фильмов, что соответствует нашему запросу. Давайте скопируем это выражение xpath и вернемся обратно в VS Code. В нашем коде давайте создадим новую переменную, назовем ее movies. И используя наше дерево элементов и метод xpath в качестве аргумента укажем наше xpath выражение в кавычках. Сохраняем. Значит эта переменная представляет собой список, который будет содержать все элементы типа tr, из которых нам нужно получить содержимое. Давайте сейчас создадим цикл, в котором будем проходить по всему списку. Для каждого фильма в списке фильмов это у нас заготовка. Теперь нам нужно опять вернуться в Chrome. В Chrome здесь у нас первый td элемент, наводим на него мышкой и видим, что подсвечивается первый столбец в таблице. Второй элемент td это второй элемент или второй столбец в таблице. Внутри второго тега td есть тег a, который кроме прочего и содержит текстовое значение, а именно название фильма. Давайте получим к нему доступ с помощью xpath. Дописываем наше выражение. В следующем тег после tr у нас td. Нам нужен элемент td, значение атрибута класса которого равно title column. Поэтому в квадратных скобках указываем класс и его значение. В нашем случае это title column, можно его скопировать. Мы получили 100 таких элементов, нам же нужен один. Поэтому внутри этого элемента находим tg a и из него можем извлечь текст. Копируем наше xpath выражение и возвращаемся в vias code. Теперь внутри цикла добавим ключ. Мы извлекаем название фильма, поэтому назовем его name. И находим внутри каждой переменной movie с помощью xpath название фильма. Но поскольку нам не нужен полный путь, мы его можем скопировать. Но часть из этого пути уже содержится в самом movie. Поэтому мы оставляем только ту часть которая идет после tr. Начиная с тега td, все предыдущее можно удалить. Ставим здесь точку перед двумя слешами. И нам нужно получить доступ только к первому элементу, списка. Сохраняем. Возвращаемся дальше в Chrome. Теперь нам нужно получить год выпуска фильма. Воспользуемся инструментом поиска. Наведем на год выпуска. Щелкаем по нему. И как видите, год выпуска содержится в теге span. Спан пропишем xpath до года выпуска. Итак, у нас год выпуска находится в t-body tr. Но начиная с td мы должны изменить xpath выражение. И обратиться к тегу span. Найти txpan. Как видите, их много. Давайте еще раз посмотрим на него. Нам нужен txpan значение атрибута класса, которого равно secondary info. Поэтому в квадратных скобках пишем class. Равно secondary info. Все, мы получили доступ к году выпуска фильма. Выделяем и копируем xpath выражение и возвращаемся обратно в код. В коде создадим новый ключ. Назовем его releaseEA. Также обращаемся к movie с помощью метода xpath. И прописываем xpath выражение, которое мы скопировали. Давайте его отредактируем аналогичным образом. Мы начинаем поиск начиная с тега td. Здесь не забываем ставить точку. И обращаемся только к нулевому элементу. Да, мы не создали словарь, поэтому давайте сейчас это сделаем. И все наши данные будем записывать в словарь. Назовем его просто m. И все, что мы написали, завернем в фигурные скобки. Те же действия нам нужно повторить и для остальных элементов на веб-странице. Сейчас я предлагаю вам поставить видео на паузу. И постараться самостоятельно выполнить поиск остальных элементов. А именно, изменение позиции в рейтинге. И также значение title.metra. Сейчас я вернусь на страницу и покажу, что это. Значение title.metra на этой странице это вот эта стрелочка. Это стрелочка, которая указывает в какую сторону изменился рейтинг. Вверх или вниз. Давайте воспользуемся инструментом поиска. Наведем на него. И в коде мы видим, что здесь title.metra.up. То есть рейтинг вырос. Давайте посмотрим, что в других элементах. Здесь title.metra.down. Попробуйте извлечь и само слово down из этого элемента. Итак, ставьте видео на паузу и возвращайтесь через некоторое время. Итак, надеюсь у вас все получилось. Вы можете свериться с кодом, приведенным на экране или в конспекте коллекции. Теперь давайте создадим пустой список. Зовем его allmovies. Который будет содержать все наши данные. И в цикле будем записывать в этот список словарьи. И на этом пока все. Давайте выведем наш список и его длину. Сохраним скрипт и запишем его. Итак, мы получили список словарей. И общую длину элементов списка равную 100. Что соответствует количеству словарей. И мы получим стират. В этом списке мы получим список словарей. И мы получим его по ссылке в описании. равную 100, что соответствует количеству фильмов на целевой странице. Если вы присмотритесь к результату скрейпинга, то можете увидеть, что некоторые поля содержат специальные символы, некоторые поля могут содержать лишнюю информацию. Поэтому, следующая ваша задача, следующий шаг это выполнить парсинг элементов с помощью тех методов, которые мы уже изучали на том уроке, когда проходили Beautiful Soup и на семинарах. Поэтому сейчас я не буду останавливаться подробно на том, как извлекать информацию из каждого элемента списка и приведу сразу готовый код. Итак, я вам конечно рекомендую самостоятельно выполнить парсинг каждого элемента для извлечения точно той информации, которая вам нужна. Здесь же в частности, например, год выпуска преобразован в тип INT, в целое число. Здесь позиция также преобразована в целое число. Также мы извлекаем только ту информацию, которая нам нужна. Наконец, следующий этап это добавление данных в MangoDB. Давайте напишем функцию для добавления полученных данных в локальное хранилище MangoDB. Для этого импортируем MangoClient и SpyMango и напишем функцию, назовем ее insertToDB, в которую будем передавать список фильмов. Подключаемся к локальному клиенту. Далее создаем в нашей базе данных новую базу, назовем ее IMDB Movies и коллекцию, назовем ее Top Movies. И наконец воспользуемся методом insertMany для добавления нашего списка. И наконец закроем клиент. После цикла вызовем нашу функцию и передадим ей список, что мы получили в результате скрепинга. Сохраняем скрипт, очистим терминал и прежде всего давайте перейдем в MangoCompass, чтобы посмотреть, что там в настоящее время базы данных нет. Вам нужно локально запустить MangoDB, вы можете убедиться, что сейчас базы данных с названием IMDB Movies у нас нет. В общем-то наша задача сейчас сюда перенести данные. Вернемся в код. Наконец, прежде чем мы запустим наш скрипт, я хотел бы обратить ваше внимание еще на две функции, которые я сюда добавил. Первая из этих функций извлекает изменение titleMetra, то есть повысился рейтинг фильма или понизился рейтинг фильма или остался без изменений. И вторая функция получает изменение позиции, то есть насколько пунктов изменился рейтинг. Итак, пробуем запускать скрипт. Ждем ответ сервера. Скрипт выполнился и давайте перейдем в MangoDB. Мы можем обновить список баз данных, если вновь созданная база данных все еще не отображается у вас в списке. Видим, что у нас появилась IMDB Movies, наша новая база Можем развернуть ее внутри увидеть коллекцию Top Movies. Давайте войдем в нее. Здесь мы видим, что у нас имеется 100 элементов, что соответствует количеству фильмов на целевой странице. И конечно все элементы, которые мы заполучили с рейтинга IMDB, представлены в базе данных. В заключение, я хочу сказать, что парсинг и скрипинг HTML с помощью LXML и XPath это мощная техника извлечения данных с веб-сайта. Библиотека LXML предоставляет быстрый и эффективный способ парсинга HTML, а XPath позволяет гибко и точно выбирать элементы и атрибуты подобных документов. Комбинируя эти инструменты с Python и другими библиотеками анализа данных, можно собирать и обрабатывать большие объемы данных из интернета для использования в исследованиях, анализе или других приложениях. На этом урок окончен и до встречи на следующей лекции. Редактор субтитров А.Синецкая Корректор А.Егорова']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bccf4f6-373c-4fad-934a-e2691dada10e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.688039Z",
     "iopub.status.busy": "2023-11-25T08:57:05.687524Z",
     "iopub.status.idle": "2023-11-25T08:57:05.709686Z",
     "shell.execute_reply": "2023-11-25T08:57:05.708545Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.687992Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = [\". \".join(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ec7446-de6d-4536-8128-233a4e1ad55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.711920Z",
     "iopub.status.busy": "2023-11-25T08:57:05.711005Z",
     "iopub.status.idle": "2023-11-25T08:57:05.733485Z",
     "shell.execute_reply": "2023-11-25T08:57:05.732362Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.711863Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "srt_text = open(\"new_test_results/new_test_text_1.srt\", mode='r').read()\n",
    "\n",
    "srt_data = []\n",
    "for line in srt_text.split('\\n\\n'):\n",
    "    index, time, sentence = line.split('\\n')\n",
    "    time_start, time_end = map(float, time.split(' --> '))\n",
    "    srt_data.append([time_start, time_end, sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "887c15fa-1ff6-44a6-b4f6-c3987f28bf66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.735640Z",
     "iopub.status.busy": "2023-11-25T08:57:05.734800Z",
     "iopub.status.idle": "2023-11-25T08:57:05.747935Z",
     "shell.execute_reply": "2023-11-25T08:57:05.746838Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.735587Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "srt_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc005214-6f46-4fae-87a4-c11f8f9bb274",
   "metadata": {},
   "source": [
    "## Подготовка батчей текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ea81fd9-9093-457b-8874-61bcf24c8f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.749936Z",
     "iopub.status.busy": "2023-11-25T08:57:05.749358Z",
     "iopub.status.idle": "2023-11-25T08:57:05.762260Z",
     "shell.execute_reply": "2023-11-25T08:57:05.761142Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.749894Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = text[0].replace(\"!\", '.').replace('?', '.')\n",
    "lines = text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb373a7-ddd0-4e36-9d1a-7d5635de9cb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.766733Z",
     "iopub.status.busy": "2023-11-25T08:57:05.765781Z",
     "iopub.status.idle": "2023-11-25T08:57:05.780477Z",
     "shell.execute_reply": "2023-11-25T08:57:05.779426Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.766697Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_symbols = 2000\n",
    "inputs = [\"\"]\n",
    "for sentence in lines:\n",
    "    if len(inputs[-1]) + len(sentence) < MAX_symbols:\n",
    "        inputs[-1] = inputs[-1].replace('\\n', '') + sentence\n",
    "    else:\n",
    "        inputs.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c092aa30-2b4c-4701-ab86-7ae57f80ebca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.783254Z",
     "iopub.status.busy": "2023-11-25T08:57:05.782430Z",
     "iopub.status.idle": "2023-11-25T08:57:05.799534Z",
     "shell.execute_reply": "2023-11-25T08:57:05.798245Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.783199Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Всем привет Сегодня мы продолжаем изучать веб скрейпинг и перейдем к разбору Python модуля LXML и языка xpath В одном из предыдущих уроков мы познакомились с Beautiful Soup и использовали его для скрейпинга HTML страниц В текущем же уроке будем работать с модулем LXML Beautiful Soup богаче с точки зрения функциональных возможностей, но для выполнения простых задач можно использовать LXML Сегодня наша задача научиться писать выражение xpath, которые используются для скрейпинга нужных частей информации В первой половине урока мы займемся теоретическим разбором инструментов, а во второй части урока применим на практике полученные знания и выполним скрейпинг сайта IMDB Я советую вам по ходу лекции повторять все, что я делаю Откройте VS Code и воспроизводите тот же код, что и в лекции Итак, сегодня на уроке мы разберем основы LXML, xpath, поработаем с xpath в рамках LXML, поговорим о том, что такое CSS селекторы, и наконец выполним скрейпинг сайта с помощью xpath Основы LXML Начнем с определения LXML – это библиотека Python для обработки документов XML и HTML Она предоставляет быстрый и эффективный API для парсинга, манипулирования и серилизации данных XML и HTML Основные возможности LXML включают поддержку xpath и работу с деревом элементов Давайте перейдем в нашу среду разработки Visual Studio Code и продолжим работать там Мы находимся в VS Code и для начала нужно установить LXML Установка LXML стандартная, посредством PIP Установка LXML У меня эта библиотека уже установлена, поэтому продолжаем работу дальше Чтобы начать работу с LXML, давайте возьмем элементарную HTML страницу и посмотрим на ее структуру Как видно, тело страницы содержит один параграф, hello GeekBrains, один список, внутри которого находятся элементы, и второй из элементов содержит тег гиперссылки Можно запустить этот файл прямо из среды разработки Для этого нужно установить расширение, которое называется Live Server Для этого перейдем на вкладку расширения В строке поиска напишем Live Server'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6da5a-d0ed-4751-b107-f389b50dd3e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Суммаризация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe6281ea-1d51-4d53-8d11-85bea7f1400e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.801977Z",
     "iopub.status.busy": "2023-11-25T08:57:05.801173Z",
     "iopub.status.idle": "2023-11-25T08:57:05.819003Z",
     "shell.execute_reply": "2023-11-25T08:57:05.817889Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.801941Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model_name,\n",
    "    input_records,\n",
    "    output_file,\n",
    "    max_source_tokens_count=600,\n",
    "    batch_size=8\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    for batch in tqdm(input_records, total=len(input_records)):\n",
    "        texts = batch\n",
    "        input_ids = tokenizer(\n",
    "            texts,                                                                                                     \n",
    "            add_special_tokens=True,\n",
    "            max_length=max_source_tokens_count,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].to(device)\n",
    "        \n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            no_repeat_ngram_size=4\n",
    "        )\n",
    "        summaries = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        for s in summaries:\n",
    "            print(s)\n",
    "        predictions.extend(summaries)\n",
    "    with open(output_file, \"w\") as w:\n",
    "        for p in predictions:\n",
    "            w.write(p.strip().replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "# gazeta_test = load_dataset('IlyaGusev/gazeta', script_version=\"v1.0\")[\"test\"]\n",
    "output_file = \"t5_predictions.txt\"\n",
    "# predict(\"IlyaGusev/rut5_base_sum_gazeta\", inputs, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aafac55-0305-4702-8c68-d8a0cc1119db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.821298Z",
     "iopub.status.busy": "2023-11-25T08:57:05.820282Z",
     "iopub.status.idle": "2023-11-25T08:57:05.834152Z",
     "shell.execute_reply": "2023-11-25T08:57:05.832974Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.821247Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batched_summary_text = \"\"\n",
    "with open(output_file, mode='r') as f:\n",
    "    batched_summary_text = f.readlines()\n",
    "batched_summary_text = \" \".join(batched_summary_text)\n",
    "\n",
    "print(len(batched_summary_text))\n",
    "    \n",
    "model_name = \"IlyaGusev/rut5_base_sum_gazeta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    [batched_summary_text],\n",
    "    max_length=6293,\n",
    "    add_special_tokens=True,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")[\"input_ids\"]\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    no_repeat_ngram_size=4\n",
    ")[0]\n",
    "\n",
    "summary = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e655e-c544-41ac-8f0a-37cb532226c7",
   "metadata": {},
   "source": [
    "## Выделение терминов из текста используя Saiga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f8062-a185-4192-a741-11f98a4b46fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1426a55a-af94-44c1-afd8-c1ef73faad5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T08:57:05.836432Z",
     "iopub.status.busy": "2023-11-25T08:57:05.835518Z",
     "iopub.status.idle": "2023-11-25T09:01:40.647090Z",
     "shell.execute_reply": "2023-11-25T09:01:40.645844Z",
     "shell.execute_reply.started": "2023-11-25T08:57:05.836378Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab0b4937e844485b7566eef38cfb2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_config.json:   0%|          | 0.00/419 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1950ab681cbc45e0977997040f3262c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094957b72ca84d798932ba75adad1786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46e4a69041e4087a87a23976e390f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f11e3baef6147e1800c29bdb95e2cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1d5739a12f424ea9f539f2a6e84a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1295c3aad5641b893167e33d7ead47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efaf1b603cd4b18ae08ca6c85d6eecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c9abd067424fff8ca6dfc69861d879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_model.bin:   0%|          | 0.00/67.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b791c92525be4d3983ae175a27c1408b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/334 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b0619cdd0a4110ab422aeae38e100f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ff29836d0d4418bc16c27535c2ca48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee29a14798a44e883de0bea8e2d2e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 1536,\n",
      "  \"no_repeat_ngram_size\": 15,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.01,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"IlyaGusev/saiga_7b_lora\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.temperature = 0.01\n",
    "print(generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9e2ca-0b34-42cf-aa81-ed8d35002b99",
   "metadata": {},
   "source": [
    "формирование терминов по батчам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0879e28f-dbf0-45eb-b36a-c195a5833e21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:01:40.650392Z",
     "iopub.status.busy": "2023-11-25T09:01:40.648819Z",
     "iopub.status.idle": "2023-11-25T09:07:13.012907Z",
     "shell.execute_reply": "2023-11-25T09:07:13.011656Z",
     "shell.execute_reply.started": "2023-11-25T09:01:40.650336Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fe0e91e44e411990f4f6f4918bd956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LXML - это библиотека Python для обработки документов XML (Extensible Markup Language) и HTML (Hypertext Markup Language). Она предоставляет быстрый и эффектный API для парсинга, манипулирования и серализации данных XML и HTML. Основные возможности LXML включают поддержку xpath и работы с деревом элементов.\n",
      "\n",
      "==============================\n",
      "\n",
      "HTML и XML\n",
      "\n",
      "==============================\n",
      "\n",
      "HTML тег head, тег body, тег p, тег a.\n",
      "\n",
      "==============================\n",
      "\n",
      "Пример ответов:\n",
      "1. a и b - это два сложных термина, которые лектор давал определение.\n",
      "2. a и b - это два сложных термина, которые относятся к теме урока.\n",
      "\n",
      "==============================\n",
      "\n",
      "HTML документ рассматривается как дерево узлов. В xpath мы можем выбрать элемент, используючи двойной слэш и затем имя элемента.\n",
      "\n",
      "==============================\n",
      "\n",
      "Для выполнения задания необходимо выполнить следующее:\n",
      "- Посмотреть на список элементов на странице и выбрать все дивы.\n",
      "- Использовать двойной слэш div для выбора элементов по их атрибуту, ID или классу.\n",
      "- Добавить две квадратные скобки, следующие за имене элемента, а затем значение атрибута.\n",
      "- Выбрать элемент на основе его позиции.\n",
      "- Выбрать первый элемент списка UL, используя функцию position.\n",
      "- Использовать функцию contains в xpath для выбора только первого элемента списка и этого первого элемента списка должен содержать текст \"hello\".\n",
      "- Использовать функцию osi для перемещения в дереве HTML вверх или вниз, используя ось xpath.\n",
      "- Использовать функцию parent для получения родителя указанного узла.\n",
      "- Использовать функцию preceding для получения всех узлов, которые появляются перед текущим узлом за искомый диапазон времени.\n",
      "- Использовать функцию preceding sibling для получения братьев определенного элемента, то есть все эlements одного уровня до текущего узла.\n",
      "\n",
      "==============================\n",
      "\n",
      "Для решения задания необходимо использовать язык программирования Python и библиотеку BeautifulSoup.\n",
      "\n",
      "Перед началом работы необходимо подключиться к интернету и открыть страницу с HTML-кодом.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Подключение к интернету\n",
      "url = 'https://www.google.com'\n",
      "response = requests.get(url)\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "\n",
      "# Выбор тегов\n",
      "tag_title = soup.find('h1', {'class': 'r'})\n",
      "tag_body = soup.find('div', {'id': 'cse'))\n",
      "\n",
      "# Парсинг\n",
      "print(tag_title.text) # Вывод \"Google\"\n",
      "print(tag_body.text) # Вывод \"Расписание мероприятий\"\n",
      "```\n",
      "\n",
      "Обратите внимание, что мы используем класс `r` для выбора тега `<h1>`, так как он является единственным тегом с таким классом на странице. Также мы используем ID `cse` для выбора тега `<div>` с текстом.\n",
      "\n",
      "После того, как мы получили тексты, мы можем вывести их на экран или сохранить в файл.\n",
      "\n",
      "Надеюсь, это поможет вам решить задание! Если у вас есть какие-либо вопросы, не стесняйтесь задавать их мне.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для того, чтобы получить текст, который находится внутри элементов списку, мы можем использовать метод xpathtext. В цикле for мы выберем все элементы li и применим к каждому из них метод xpathtext. Затем мы создадим переменную text, которая будет брать каждыый элемент li, применять к нему метод xpathtext и сохранить результат в переменной text.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup as soup\n",
      "\n",
      "url = 'https://www.example.com/html-document'\n",
      "response = requests.get(url)\n",
      "soup = soup(response.content, \"html.parser\")\n",
      "\n",
      "# создаем переменную ListItems, которую будем использовать для хранения списка элементов li\n",
      "list_items = []\n",
      "\n",
      "for li in soup.find_all('li'):\n",
      "    list_items.append(li)\n",
      "\n",
      "# создаем переменную text, которая будет брать каждый элемент li, применять к нему метод xpathtext\n",
      "text = []\n",
      "\n",
      "for item in list_items:\n",
      "    text.append(item.xpath(\"./a/@href\").extract())\n",
      "\n",
      "print(text[0]) # вывести первый элемент списка\n",
      "print(text[1]) # вывести второй элемент списка\n",
      "```\n",
      "\n",
      "После выполнения кода мы получим два списка элементов, содержащих полную html размерку. Чтобы получить текст, который находится внутри эlementов списка, мы можем использовать метод xpathtext. В циклах for мы выберем все элементы li и применим их к методу xpathtext. Затем мы создадим переменную text и применим к каждому элементу li метод xpathtext и сохраним результат в переменную text.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import beautifulsoup\n",
      "\n",
      "url = 'https://www.example.com/html'\n",
      "response = requests.get(url)\n",
      "soup = beautifulsoup(response.content, 'html.parser')\n",
      "\n",
      "# создаем переменную ListItems, которую будут использовать для хранения списка элементов li\n",
      "list_elements = []\n",
      "\n",
      "for element in soup.select('.//li'):\n",
      "    list_elements.append(element)\n",
      "\n",
      "# создаем переменную text, которая будет применяться к каждому элементу li, применять метод xpathtext и сохранять результат в переменную text\n",
      "text = []\n",
      "\n",
      "for element in list_elements:\n",
      "    text.append(element.xpath(\".//a/@href\").text)\n",
      "\n",
      "print(text[0]) # вывести первый элемент списка\n",
      "print(text[1]) # вывести второй экземпляр списка\n",
      "```\n",
      "\n",
      "После выполнения кода мы получаем два списка, первые из которых содержат только один элемент с текстом, а второй содержит три элемента. Однако все еще мы получаем пробелы. Чтобы это исправить, мы можем использовать метод map, который принимает в числе первого аргумента функцию, которая будет применена ко всем элементам списка.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulStoneSoup as soup\n",
      "\n",
      "url = 'https://www.example2.com/html'\n",
      "response = requests.get(url) \n",
      "soup = BeautifulStoneSoup(response.content, 'html.parser')\n",
      "\n",
      "# выбираем все элементы li и применяем к ним метод xpathtext\n",
      "list_elements = [x for x in soup.select('.//li') if x.xpath(\".//a/@href\")]\n",
      "\n",
      "# создаем переменную text, которая будет принимать в себя результат метода xpathtext и сохранять его в переменную text\n",
      "text = []\n",
      "\n",
      "for element in soup.select('.//li'): \n",
      "    text.append(element.xpath(\".//a/@href\"))\n",
      "\n",
      "print(text[0]) # вывести первый элемнт списка\n",
      "print(text[1]) # вывести второе элемент списка\n",
      "\n",
      "# создаем переменную text, которая будет применить метод strip к каждому элементу списка\n",
      "striped_text = []\n",
      "\n",
      "for element in list_elements:\n",
      "    striped_text.append(element.xpath(\".//a/@href\").strip())\n",
      "\n",
      "print(striped_text[0]) # вывести первый элемент списка без пробелов\n",
      "print(striped_text[1]) # вывести второй элемент списка без пробелов\n",
      "```\n",
      "\n",
      "После выполнения кода мы получим только текст со всех специальных символов, находящихся в html коде. Чтобы это исправить, мы можем использовать функцию xpathtext, которая принимает в качестве первого аргумента функцию, которая применяется к каждому элементу списка.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4.BeautifulStoneSoup import BeautifulStoneSoup\n",
      "\n",
      "url = 'https://www.example3.com/html'\n",
      "response = requests.get(url)  \n",
      "soup = BeautifulStoneSoup(response.content, \"html.parser\")\n",
      "\n",
      "# выбираем все элементы li и применяют к ним метод xpathtext\n",
      "list_elements = soup.select('.//li')\n",
      "\n",
      "# создаем переменную text, которая будет привязывать каждому элементу списка функцию xpathtext\n",
      "text = []\n",
      "\n",
      "for element in list_elements:\n",
      "    \n",
      "    text.append(element.xpath(\".//a/@href\", function=lambda x: x.strip()))\n",
      "\n",
      "print(text[0]) # вывести первый элекмент списка\n",
      "print(text[1]) # вывести второй элемент списка\n",
      "```\n",
      "\n",
      "После выполнения скрипта мы получим только текст со всех двух тегов, содержащих полную html разметку. Чтобы получить текст, который находится внушает внушает, используя метод xpathtext, и применяем метод xpath, и применяем метод xpath, используем метод xpath, и приобретены текст, и выводить значения, и приобретены текст, и выводить, и текст, и выводить текста\n",
      "\n",
      "==============================\n",
      "\n",
      "HTML, CSS\n",
      "\n",
      "==============================\n",
      "\n",
      "CSS SELECT и xpath это два разных языка запросов, которые используются для извлечения данных из документов HTML и XML. Оба языка служат схожим целям, но имеют некоторые отличия. CSS SELECT более мощный и гибкий язык запросов, который позволяет использовать более сложные выражения и поддерживать более широкий спектр типов данных и функций. XPath, с другой стороны, более простой и лаконичный язык запросов, который может быть полезен для задач вебскрейпингов, связанных с выбором элементов на основе своих классов, IDs или других свойств. CSS SELECT может быть полезен для задач вебскрейпы, связанных с выбором элементов на основе своего класса, ID или других свойств. Он может быть быстрее и эффективнее для определенного типа выбора, особенно при выборе на основе арифметических свойств, таких как класс или ID. Однако, если вам нужен более сложный и гибкий язык запросов, то xpath может быть лучшим выбором.\n",
      "\n",
      "==============================\n",
      "\n",
      "XML и XPath\n",
      "\n",
      "==============================\n",
      "\n",
      "HTML - язык разметки документа, используемый для создания веб-страниц.\n",
      "LXML - XML-подобный язык, используемый для работы с HTML-документами.\n",
      "\n",
      "==============================\n",
      "\n",
      "HTML, CSS, JavaScript\n",
      "\n",
      "==============================\n",
      "\n",
      "Это задание не имеет ответов, так как оно является вопросом для обсуждения.\n",
      "\n",
      "==============================\n",
      "\n",
      "Лектор: \"Давайте рассмотрим термин 'понятие'. Понятие - это идея, концепция, представление о чем-то, которое мы можем представить себе. Оно может быть связано с реальностью или нести символическое значение.\"\n",
      "\n",
      "==============================\n",
      "\n",
      "Можно использовать следующие термины:\n",
      "- Скрипты\n",
      "- Парсинг\n",
      "- МангоDB\n",
      "- Добавление данных в базу данных\n",
      "\n",
      "==============================\n",
      "\n",
      "Лектор: Добрый день! Сегодня мы будем изучать термин \"интернет\". Интернет - это сеть компьютеров, которая связывает мир. Он используется для передачи информации, для общения между людьми, для работы и многого другого. Кроме того, он является одним из основных источников знаний и образования.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inputs = [\"\"\"Выдели из текста сложные термины, которым в тексте дается определение: Друзья, привет, с вами снова я Денис. Рата с приветствовать на лекции, сегодня мы будем говорить о массиву. На прошлой лекции мы познакомились с языком программирования сишар, с его основными характеристиками, а также решили блок базовое задачу. На семинарах выпа протековались разработки циклических условных конструкций применяли арефметические операторы, а также операции вода вывода. Сегодня мы вспомним о основной и понятия связанные с массивами и характеристики, поговорим о том, в каких задачах массивы могут применяться. Познакомимся, как осуществляется операция, создание заполнения, а также выводы массива в наекрану. И решим блок задач на их обработку. Ближе концу лекции мы познакомимся с двумя разновидности мексиклов, фор и форюч, а также друзья поговорим о таком важном аспекте, как изучение английского для программистов. Друзья, перейти, как приступить к практике, и начать решать задачи, давайте вспомним о, что из себя представляет массив и какими характеристиками он обладает. Начнем с ключевого терминной оталекции. Массив. Массив представляет собой структуру данных, которая, приночена для охранения элементов, как правило, одного типа. Массив может состоять из целых чейсил веществных, может состоять из символов, а также например из строк. Если приводить примеры из реальной жизни, то массива может назвать упорядочный набор инструментов или же набор величных игрушек, который также обладает характеристикой упорядочности. Второй важный термин – это индекс элементомассива. Друзья, этот термин можно сформулировать по разному приведем наиболее популярные формулировки. Итак, первое – это смещение элемента относительно начала массива. То есть, насколько позиции элемент смещен, да, относительно его левые границы. Но и вторая формулировка наиболее простая – это просто позиция элементов массива. Друзья, напомню, что в большинстве языков программирования индекс смассива начинается с нуля. Передем к основным характеристика массивов. Первое характеристика – это то, что массив должен иметь имя, еще его называют инденцификатр. С помощью имени мы осуществляем обращения климента массива. Если нам нужно изменить элемент или же просто его прочитать, то мы используем операцию обращения климентам по индексу. Также, друзья, то операцию называют индексацию. В большинстве языков программирования она обозначается с помощью пары кванратных скобок. В привенере приведел массив из пяти элементов и не же вы можете видеть обращения к каждому его элементу с помощью оператора кванратной скобки. Отметья важную характеристику, которая пределяет внутренний устройство массив. В случае элементы в массиве располагаются последовательные, то есть, в памяти они занимают ячейку за ячейкой. Невкому случае элементы в массиве не расположены хатично или же в каком либо другом порядке. Ну и в заключении добавим, что массивы имеют конечный размер, поскольку объем памяти у нас всегда ограничен. Итак, после небольшого экскурса в теорию, давайте посмотрим как будут выглядеть основные операции при работе с массивами на языке сишару. Рассмотрим, создание, заполнение и вывод массива на экрана. В языке сишару создание массива состоит из трех остальных этапа. Первый этап – это указание типа данных для элемент о массива. Также, друзья, мы указываем оператор квадратной скобки, чтобы показать, что у нас именно создается массив, а необычное переменное. Второе этап – это указание и динсфекатра. Или же, именем массива. Вот это нам примере, друзья, мы пользуемся именем «А» – что является сокращением от «Рей». Друзья, рекомендую давать массивом осмысленные именно. В самом простом случае, можно дать имя «Рей». Однако, рекомендую задуматься над более осмысленным именаванием. Как приединенных примерах? Если вы оперируете набором «Файлов», то такое массив можно назвать именем «Файлс».  Выпиши темины с их определением используя индексацию\"\"\"]\n",
    "\n",
    "# inputs = ['Классический маркетинг можно разделить на 6 важных этапов. Во-первых, это идея. Идея нового продукта, нового проекта, адаптация существующих продуктов на рынке. Это все то, что привносит новое в рынок, либо это какое-то дополнение, возможно, к существующему продукту, либо новая функция, новое свойство. Все, что нам улучшит продукт, либо улучшит бизнес. Это, безусловно, исследование, обзор рынка, понимание, какие игроки на рынке существуют, как они взаимодействуют друг между другом, какие есть моменты влияния игроков между собой, какие, возможно, есть конкурентные факторы преимущества. Далее после исследования продукт разрабатывается с учетом идеи первоначальной, с учетом исследований, возможно, исследования повлияли на первоначальную идею. Далее начинается процесс разработки. Процесс разработки – это очень интересный процесс, который должен учитывать и идею, и соответственно исследования, которые были ранее проведены. Исследования могут повлиять на первоначальную идею, и это нужно, безусловно, учитывать в разработке продукта. Далее тестирование и производство. Когда мы с вами произвели первую партию продукта, или вообще первый продукт, будь то физический, либо услуги, либо цифровой продукт, нужно его протестировать и понять, действительно ли тот продукт, который придумали, который основан на идее и на исследованиях, будет хорошо продаваться, потому что в любом исследовании и в любой идее, в любых цифрах бывают погрешности. И в дальнейшем мы начинаем уже распространять продукт. Это дистрибуция, это дистрибуция по каналам продаж, которые важны для клиентов, аналитика этих каналов, продаж, дистрибуции, и соответственно, продвижение в каналах дистрибуции. Как только мы достигли нужного уровня распространения, мы переходим к стимулированию спроса и удержанию потребителя. И здесь, конечно, в ход идут все рекламные мероприятия, рекламные активности, и также коррекция сервиса, если он присутствует в вашем продукте. На этапе стимулирования спроса очень важно получать обратную связь от рынка, и соответственно с ней работать, чтобы регулировать спрос в лучшую сторону. Подумаем о плюсах и минусах классического маркетинга. Возьмем в качестве примера конфеты Lindt, физический продукт, который известен во всем мире в течение длительного времени. У него есть разнообразие вкусов, собственные бутики. Нет человека, который не знает конфет Lindt или шоколада Lindt. Высокая дистрибуция, свои магазины, очень широкая диджитал поддержка и очень широкая оффлайн поддержка. И, безусловно, торговая поддержка, это и мерчендайзинг, и промо. А если один из вкусов не понравится потребителю? Компания проводила исследования, собирала данные по рынку, и отдел инноваций прогнозировал объемы, создавал новый вкус. Целая команда была вовлечена в то, чтобы создать вкус, который по тем или иным причинам не был принят рынком и потребителем, несмотря на исследования, несмотря на идею и несмотря на качественную разработку. Тогда компания будет вынуждена сделать следующее. Компания должна будет заново перезапустить исследования, либо проанализировать с иной точки зрения те исследования, которые они делали первоначально. Должны будут разрабатываться новые вкусы, а старый вкус будет распродаваться либо из эмоций, из продажи.'] \n",
    "outputs = []\n",
    "\n",
    "for inp in tqdm(inputs):\n",
    "    # task = \"\"\"\n",
    "    # Выдели из текста сложные термины, которым ведущий даёт описание в лекции(как минимум одно предложение), самому придумывать НЕЛЬЗЯ, брать определения только из ТЕКСТА, \n",
    "    # записывая их в глоссарий, где описаны сложные темы в лекции. Чем меньше терминов, тем лучше.\n",
    "    # \"\"\"\n",
    "    \n",
    "    # task = \"\"\"\n",
    "    # План спринта / модуля / урока по частям\n",
    "    # Перечислите названия подразделов урока, и описание в одном предложении того, что и зачем вы в них будете рассказывать.\n",
    "    # Для этого можно воспользоваться формулировками образовательных результатов из программы, но переформулировать их на языке, \n",
    "    # понятном студенту ли обозначить темы и тезисы.\n",
    "    # \"\"\"\n",
    "    # task = 'Продолжи предложение: А теперь закрепим новые термины, которые я сегодня объяснял на уроке, давайте перечислим их, не говорите того, на чем я не делал акцент в своем докладе, всё должно относиться к сегодняшней теме, иначе я убью вас:'\n",
    "    task = 'ИЗ ТЕКСТА выпиши ДВА сложных термина, которым давал определение лектор. Отсортируй по важности, выбери только те термины, которые относятся к теме урока. Выбирай только два самых важных термина. Не фантазируй. '\n",
    "    # task = 'Выпиши из текста термины с их опредлениями. Если в тексте нет определений, пиши \"Нет\". В тексте не больше трех определений.\\nТекст: '\n",
    "    inp = inp + task\n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    # print(prompt)\n",
    "    print(output)\n",
    "    outputs.append(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8d50f-2183-4031-80a2-289174a76cf6",
   "metadata": {},
   "source": [
    "перевод терминов из текста в словарь "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a842171-f958-41fd-9631-4d920a7741db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:09:54.713574Z",
     "iopub.status.busy": "2023-11-25T09:09:54.712857Z",
     "iopub.status.idle": "2023-11-25T09:09:54.732747Z",
     "shell.execute_reply": "2023-11-25T09:09:54.731608Z",
     "shell.execute_reply.started": "2023-11-25T09:09:54.713539Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminology = dict()\n",
    "poor_symbols = [*[str(x) + '. ' for x in range(20)], '- ']\n",
    "for output in outputs:\n",
    "    collected = 0\n",
    "    for term in output.split('\\n'):\n",
    "        if len(term) == 0 or ' - ' not in term:\n",
    "            continue\n",
    "\n",
    "        for symb in poor_symbols:\n",
    "            term = term.lstrip(symb)\n",
    "\n",
    "        term, definition = term.split(' - ', maxsplit=1)\n",
    "        term = term.strip().lower()\n",
    "        term = term.strip('*')\n",
    "        if term.startswith('определение: '):\n",
    "            term = term.lstrip('определение: ')\n",
    "        term = term.lstrip(') ').strip('\"')\n",
    "        if '(' in term and ')' in term:\n",
    "            term = term[:term.rindex('(')]\n",
    "        term = term.strip()\n",
    "        if term.startswith('ответ: '):\n",
    "            term = term.lstrip('ответ: ')\n",
    "        if 'a и b' in term or 'лектор: ' in term:\n",
    "            continue\n",
    "        if term.lower() not in terminology:\n",
    "            terminology[term.lower()] = definition.rstrip('.')\n",
    "            collected += 1\n",
    "        # if collected == 10:\n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8159feca-e7d4-41ec-8e21-25c3ac38e8aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:09:55.743751Z",
     "iopub.status.busy": "2023-11-25T09:09:55.742793Z",
     "iopub.status.idle": "2023-11-25T09:09:55.762029Z",
     "shell.execute_reply": "2023-11-25T09:09:55.760864Z",
     "shell.execute_reply.started": "2023-11-25T09:09:55.743706Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terminology.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30897a14-01a3-4c3f-bd85-9d85548075ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:09:56.773113Z",
     "iopub.status.busy": "2023-11-25T09:09:56.772168Z",
     "iopub.status.idle": "2023-11-25T09:09:56.792967Z",
     "shell.execute_reply": "2023-11-25T09:09:56.791784Z",
     "shell.execute_reply.started": "2023-11-25T09:09:56.773070Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['html', 'lxml']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(terminology.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74c50233-9c8f-4b3f-83ff-7f216ed88f4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:09:58.851519Z",
     "iopub.status.busy": "2023-11-25T09:09:58.850630Z",
     "iopub.status.idle": "2023-11-25T09:09:58.865510Z",
     "shell.execute_reply": "2023-11-25T09:09:58.864500Z",
     "shell.execute_reply.started": "2023-11-25T09:09:58.851478Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lxml': 'это библиотека Python для обработки документов XML (Extensible Markup Language) и HTML (Hypertext Markup Language). Она предоставляет быстрый и эффектный API для парсинга, манипулирования и серализации данных XML и HTML. Основные возможности LXML включают поддержку xpath и работы с деревом элементов',\n",
       " 'html': 'язык разметки документа, используемый для создания веб-страниц'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec3de8-136e-4c3f-bef5-4abc137da703",
   "metadata": {},
   "source": [
    "Добавляем таймстампы к терминам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8450c1ca-1e52-49cb-aaf7-13524b45ee98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:07:13.113151Z",
     "iopub.status.busy": "2023-11-25T09:07:13.112296Z",
     "iopub.status.idle": "2023-11-25T09:07:13.125138Z",
     "shell.execute_reply": "2023-11-25T09:07:13.123821Z",
     "shell.execute_reply.started": "2023-11-25T09:07:13.113100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "srt_text = open(\"new_test_results/new_test_text_1.srt\", mode='r').read()\n",
    "\n",
    "srt_data = []\n",
    "for line in srt_text.split('\\n\\n'):\n",
    "    index, time, sentence = line.split('\\n')\n",
    "    time_start, time_end = map(float, time.split(' --> '))\n",
    "    srt_data.append([time_start, time_end, sentence])\n",
    "\n",
    "timings = []\n",
    "for term in terminology.keys():\n",
    "    for time_start, time_end, sentence in srt_data:\n",
    "        if term in sentence.lower():\n",
    "            timings.append([time_start, time_end])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f22041f-5a6c-4b19-9800-d87bc79080e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:07:13.127197Z",
     "iopub.status.busy": "2023-11-25T09:07:13.126461Z",
     "iopub.status.idle": "2023-11-25T09:07:13.137765Z",
     "shell.execute_reply": "2023-11-25T09:07:13.136674Z",
     "shell.execute_reply.started": "2023-11-25T09:07:13.127145Z"
    }
   },
   "outputs": [],
   "source": [
    "# timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "976ea74e-8a9b-4baa-a1b3-1c673791e69d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:07:13.139680Z",
     "iopub.status.busy": "2023-11-25T09:07:13.139062Z",
     "iopub.status.idle": "2023-11-25T09:07:13.150248Z",
     "shell.execute_reply": "2023-11-25T09:07:13.149225Z",
     "shell.execute_reply.started": "2023-11-25T09:07:13.139648Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [x for x in list(terminology.keys()) if x in true_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cb07b-5e09-4f2a-91f8-245c0494c252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
